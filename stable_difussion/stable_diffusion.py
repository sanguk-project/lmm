import torch
from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler, StableDiffusionPipeline
from transformers import CLIPTextModel, CLIPTokenizer
from datasets import load_dataset
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm
import tqdm as tqdm_module
import warnings
import time
import numpy as np
from tabulate import tabulate
import os

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore")

# í„°ë¯¸ë„ ì¶œë ¥ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def clear_line():
    """í˜„ì¬ ë¼ì¸ì„ ì§€ìš°ëŠ” í•¨ìˆ˜"""
    print("\033[K", end="\r")

def print_table(data, headers, title=None):
    """í‘œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë¬¸ìì—´ ìƒì„±"""
    output_str = ""
    if title:
        output_str += f"{title}\n"
    table_str = tabulate(data, headers=headers, tablefmt="grid", floatfmt=".6f")
    output_str += table_str
    return output_str

# ì„¤ì •
model_id = "stabilityai/stable-diffusion-2"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ’» ì¥ì¹˜: {device}")

# ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ë¡œë“œ ì¤‘...")
vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae").to(device)
text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder="text_encoder").to(device)
tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder="tokenizer")
unet = UNet2DConditionModel.from_pretrained(model_id, subfolder="unet").to(device)
scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ì…‹ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
dataset_file_path = os.getenv('DATASET_FILE_PATH', "./datasets/captions/test_captions_ko.jsonl")

if not os.path.exists(dataset_file_path):
    raise FileNotFoundError(f"ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_file_path}")

dataset = load_dataset("json", data_files=dataset_file_path, split="train")

def load_image(example):
    example["image"] = Image.open(example["file_path"])
    example["text"] = example["caption"]
    return example

dataset = dataset.map(load_image)

# ì „ì²˜ë¦¬
preprocess = transforms.Compose([
    transforms.Resize((1024, 1024)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5]),
])

def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    texts = examples["text"]
    return {"images": images, "texts": texts}

dataset.set_transform(transform)
print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ ({len(dataset)} ì´ë¯¸ì§€)")

train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = AdamW(unet.parameters(), lr=1e-5)
print("ğŸš€ í•™ìŠµ ì‹œì‘...")

# í•™ìŠµ ë£¨í”„
num_epochs = 1  # í•„ìš”ì— ë”°ë¼ ì¡°ì •
total_steps = len(train_dataloader) * num_epochs
losses = []

# ì†ì‹¤ í‘œì‹œë¥¼ ìœ„í•œ í…Œì´ë¸” êµ¬ì„±
loss_headers = ["Step", "í˜„ì¬ ì†ì‹¤", "ì´ë™ í‰ê·  (10)", "í‰ê·  ì†ì‹¤"]
loss_history = []
loss_display_interval = 10  # ì†ì‹¤ ì •ë³´ ì—…ë°ì´íŠ¸ ì£¼ê¸°

for epoch in range(num_epochs):
    epoch_start_time = time.time()
    epoch_losses = []
    previous_total_lines = 0  # ì´ì „ í…Œì´ë¸”ì˜ ë¼ì¸ ìˆ˜ ì´ˆê¸°í™”
    
    # ê³ ì •ëœ ì§„í–‰ë°” ìƒì„±
    progress_bar = tqdm(
        total=len(train_dataloader),
        desc=f"Epoch {epoch+1}/{num_epochs}",
        bar_format='{l_bar}{bar:30}{r_bar}',
        ncols=80,
        position=0,
        leave=True
    )
    
    # í…Œì´ë¸” ì´ˆê¸°í™”
    loss_table_rows = []
    
    for step, batch in enumerate(train_dataloader):
        images = batch["images"].to(device)
        texts = batch["texts"]
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        inputs = tokenizer(texts, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")
        input_ids = inputs.input_ids.to(device)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            text_embeddings = text_encoder(input_ids)[0]
        
        # ì´ë¯¸ì§€ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©
        latents = vae.encode(images).latent_dist.sample()
        latents = latents * 0.18215  # ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜
        
        # ë…¸ì´ì¦ˆ ìƒ˜í”Œë§
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.shape[0],)).long().to(device)
        
        # ë…¸ì´ì¦ˆ ì¶”ê°€
        noisy_latents = scheduler.add_noise(latents, noise, timesteps)
        
        # UNetìœ¼ë¡œ ë…¸ì´ì¦ˆ ì˜ˆì¸¡
        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample
        
        # ì†ì‹¤ ê³„ì‚°
        loss = F.mse_loss(noise_pred, noise)
        
        # ì—­ì „íŒŒ
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        # ì†ì‹¤ ê¸°ë¡
        loss_value = loss.item()
        epoch_losses.append(loss_value)
        losses.append(loss_value)
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress_bar.update(1)
        progress_bar.set_postfix({
            "ì†ì‹¤": f"{loss_value:.4f}"
        })
        
        # ì†ì‹¤ í…Œì´ë¸”ì— ì •ë³´ ì¶”ê°€
        moving_avg_10 = np.mean(epoch_losses[-10:]) if len(epoch_losses) >= 10 else np.mean(epoch_losses)
        avg_loss = np.mean(epoch_losses)
        
        # ë‹¨ê³„ë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘
        loss_table_rows.append([step + 1, loss_value, moving_avg_10, avg_loss])
        
        # ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ í…Œì´ë¸” ì¶œë ¥
        if (step + 1) % loss_display_interval == 0 or step == len(train_dataloader) - 1:
            # ë§ˆì§€ë§‰ 10ê°œ í–‰ë§Œ í‘œì‹œ
            display_rows = loss_table_rows[-10:]
            
            # í…Œì´ë¸” ë¬¸ìì—´ ìƒì„±
            output_str = print_table(
                display_rows, 
                loss_headers, 
                title=f"ğŸ“‰ ì†ì‹¤ í˜„í™© (Epoch {epoch+1}, ë‹¨ê³„ {step+1}/{len(train_dataloader)})"
            )
            lines = output_str.split('\n')
            total_lines = len(lines)
            
            # ì´ì „ í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ì»¤ì„œë¥¼ ìœ„ë¡œ ì´ë™
            if previous_total_lines > 0:
                move_up_str = "\033[F" * previous_total_lines
                tqdm_module.write(move_up_str, end='')
            
            # ìƒˆ í…Œì´ë¸” ì¶œë ¥
            tqdm_module.write(output_str)
            previous_total_lines = total_lines
    
    progress_bar.close()
    
    # ì—í­ ìš”ì•½
    epoch_time = time.time() - epoch_start_time
    avg_loss = np.mean(epoch_losses)
    
    # ì—í­ ìš”ì•½ í…Œì´ë¸”
    epoch_summary = [
        ["ì‹œê°„ (ì´ˆ)", epoch_time],
        ["í‰ê·  ì†ì‹¤", avg_loss],
        ["ìµœì†Œ ì†ì‹¤", min(epoch_losses)],
        ["ìµœëŒ€ ì†ì‹¤", max(epoch_losses)]
    ]
    
    print(print_table(
        epoch_summary, 
        headers=["ì§€í‘œ", "ê°’"], 
        title=f"ğŸ“Š Epoch {epoch+1} ìš”ì•½"
    ))

# ìµœì¢… ìš”ì•½ í…Œì´ë¸”
final_summary = [
    ["ì „ì²´ ë‹¨ê³„", total_steps],
    ["ìµœì¢… í‰ê·  ì†ì‹¤", np.mean(losses)],
    ["ìµœì†Œ ì†ì‹¤", min(losses)],
    ["ìµœëŒ€ ì†ì‹¤", max(losses)]
]

print(print_table(
    final_summary,
    headers=["ì§€í‘œ", "ê°’"],
    title="ğŸ í•™ìŠµ ì™„ë£Œ!"
))

# ëª¨ë¸ ì €ì¥
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
pipe = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    unet=unet,
    scheduler=scheduler,
    safety_checker=None,  # ì•ˆì „ì„± ê²€ì‚¬ ë¹„í™œì„±í™”
    feature_extractor=None  # íŠ¹ì„± ì¶”ì¶œê¸° ë¹„í™œì„±í™”
)
pipe.save_pretrained("fine-tuned-model/nia/test1")
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: 'fine-tuned-model'")